{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 330,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 14.448390007019043,
      "learning_rate": 5e-05,
      "loss": 16.0797,
      "step": 1
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 14.55915355682373,
      "learning_rate": 4.984848484848485e-05,
      "loss": 15.4614,
      "step": 2
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 14.879033088684082,
      "learning_rate": 4.9696969696969694e-05,
      "loss": 14.1367,
      "step": 3
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 13.746284484863281,
      "learning_rate": 4.9545454545454553e-05,
      "loss": 14.2697,
      "step": 4
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 20.523157119750977,
      "learning_rate": 4.93939393939394e-05,
      "loss": 13.398,
      "step": 5
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 46.09311294555664,
      "learning_rate": 4.9242424242424245e-05,
      "loss": 11.9009,
      "step": 6
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 34.85245132446289,
      "learning_rate": 4.909090909090909e-05,
      "loss": 11.8713,
      "step": 7
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 26.096487045288086,
      "learning_rate": 4.8939393939393944e-05,
      "loss": 11.004,
      "step": 8
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 30.767248153686523,
      "learning_rate": 4.878787878787879e-05,
      "loss": 9.5026,
      "step": 9
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 35.924808502197266,
      "learning_rate": 4.863636363636364e-05,
      "loss": 9.6572,
      "step": 10
    },
    {
      "epoch": 1.0,
      "grad_norm": 14.035052299499512,
      "learning_rate": 4.848484848484849e-05,
      "loss": 9.3668,
      "step": 11
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 8.374699592590332,
      "learning_rate": 4.8333333333333334e-05,
      "loss": 9.1564,
      "step": 12
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 10.148764610290527,
      "learning_rate": 4.8181818181818186e-05,
      "loss": 8.9354,
      "step": 13
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 7.4304938316345215,
      "learning_rate": 4.803030303030303e-05,
      "loss": 9.0699,
      "step": 14
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 6.0691094398498535,
      "learning_rate": 4.787878787878788e-05,
      "loss": 9.026,
      "step": 15
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 6.117002964019775,
      "learning_rate": 4.772727272727273e-05,
      "loss": 8.7816,
      "step": 16
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 6.418217658996582,
      "learning_rate": 4.7575757575757576e-05,
      "loss": 9.072,
      "step": 17
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 7.0880632400512695,
      "learning_rate": 4.742424242424243e-05,
      "loss": 8.5677,
      "step": 18
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 7.45277738571167,
      "learning_rate": 4.7272727272727275e-05,
      "loss": 8.5207,
      "step": 19
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 7.035833358764648,
      "learning_rate": 4.712121212121212e-05,
      "loss": 8.1127,
      "step": 20
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 7.833856105804443,
      "learning_rate": 4.696969696969697e-05,
      "loss": 7.9664,
      "step": 21
    },
    {
      "epoch": 2.0,
      "grad_norm": 7.772507667541504,
      "learning_rate": 4.681818181818182e-05,
      "loss": 8.0011,
      "step": 22
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 6.99207067489624,
      "learning_rate": 4.666666666666667e-05,
      "loss": 7.6863,
      "step": 23
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 7.9558587074279785,
      "learning_rate": 4.651515151515152e-05,
      "loss": 7.4512,
      "step": 24
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 6.42730188369751,
      "learning_rate": 4.636363636363636e-05,
      "loss": 7.3579,
      "step": 25
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 6.6508307456970215,
      "learning_rate": 4.621212121212121e-05,
      "loss": 7.1967,
      "step": 26
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 7.407746315002441,
      "learning_rate": 4.606060606060607e-05,
      "loss": 6.9339,
      "step": 27
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 6.113040447235107,
      "learning_rate": 4.5909090909090914e-05,
      "loss": 6.9218,
      "step": 28
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 8.409398078918457,
      "learning_rate": 4.575757575757576e-05,
      "loss": 7.1191,
      "step": 29
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 6.138376235961914,
      "learning_rate": 4.5606060606060606e-05,
      "loss": 6.4809,
      "step": 30
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 6.179681301116943,
      "learning_rate": 4.545454545454546e-05,
      "loss": 6.4478,
      "step": 31
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 6.226160049438477,
      "learning_rate": 4.5303030303030304e-05,
      "loss": 6.2066,
      "step": 32
    },
    {
      "epoch": 3.0,
      "grad_norm": 6.60272741317749,
      "learning_rate": 4.515151515151516e-05,
      "loss": 5.9479,
      "step": 33
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 7.5299906730651855,
      "learning_rate": 4.5e-05,
      "loss": 6.1466,
      "step": 34
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 5.272144794464111,
      "learning_rate": 4.484848484848485e-05,
      "loss": 5.8369,
      "step": 35
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 4.810710430145264,
      "learning_rate": 4.46969696969697e-05,
      "loss": 5.6386,
      "step": 36
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 4.4893693923950195,
      "learning_rate": 4.454545454545455e-05,
      "loss": 5.3877,
      "step": 37
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 3.4978013038635254,
      "learning_rate": 4.43939393939394e-05,
      "loss": 5.3609,
      "step": 38
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 9.341917991638184,
      "learning_rate": 4.4242424242424246e-05,
      "loss": 5.1482,
      "step": 39
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 3.663256883621216,
      "learning_rate": 4.409090909090909e-05,
      "loss": 5.173,
      "step": 40
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 3.0514345169067383,
      "learning_rate": 4.3939393939393944e-05,
      "loss": 4.9924,
      "step": 41
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 2.916252613067627,
      "learning_rate": 4.378787878787879e-05,
      "loss": 5.0306,
      "step": 42
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 2.3558552265167236,
      "learning_rate": 4.3636363636363636e-05,
      "loss": 4.8847,
      "step": 43
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.558758497238159,
      "learning_rate": 4.348484848484849e-05,
      "loss": 4.8835,
      "step": 44
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 2.020850658416748,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 4.8,
      "step": 45
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 1.62380051612854,
      "learning_rate": 4.318181818181819e-05,
      "loss": 4.7396,
      "step": 46
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 2.2551326751708984,
      "learning_rate": 4.303030303030303e-05,
      "loss": 4.7054,
      "step": 47
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 1.5270260572433472,
      "learning_rate": 4.287878787878788e-05,
      "loss": 4.6,
      "step": 48
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 1.4393550157546997,
      "learning_rate": 4.2727272727272724e-05,
      "loss": 4.6248,
      "step": 49
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 1.3708558082580566,
      "learning_rate": 4.257575757575758e-05,
      "loss": 4.5997,
      "step": 50
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 1.262946605682373,
      "learning_rate": 4.242424242424243e-05,
      "loss": 4.5778,
      "step": 51
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 1.1286126375198364,
      "learning_rate": 4.2272727272727275e-05,
      "loss": 4.5593,
      "step": 52
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 1.0479373931884766,
      "learning_rate": 4.212121212121212e-05,
      "loss": 4.6253,
      "step": 53
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 0.9920751452445984,
      "learning_rate": 4.196969696969697e-05,
      "loss": 4.4024,
      "step": 54
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.8721272945404053,
      "learning_rate": 4.181818181818182e-05,
      "loss": 4.341,
      "step": 55
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 0.8257843852043152,
      "learning_rate": 4.166666666666667e-05,
      "loss": 4.3325,
      "step": 56
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 0.7811326384544373,
      "learning_rate": 4.151515151515152e-05,
      "loss": 4.3734,
      "step": 57
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 0.7503213882446289,
      "learning_rate": 4.1363636363636364e-05,
      "loss": 4.4622,
      "step": 58
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 0.7227237224578857,
      "learning_rate": 4.1212121212121216e-05,
      "loss": 4.2767,
      "step": 59
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 0.7895187735557556,
      "learning_rate": 4.106060606060606e-05,
      "loss": 4.3123,
      "step": 60
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 0.6746218204498291,
      "learning_rate": 4.0909090909090915e-05,
      "loss": 4.2666,
      "step": 61
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 0.6848980188369751,
      "learning_rate": 4.075757575757576e-05,
      "loss": 4.2104,
      "step": 62
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 0.6380462646484375,
      "learning_rate": 4.0606060606060606e-05,
      "loss": 4.1416,
      "step": 63
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 0.6094082593917847,
      "learning_rate": 4.045454545454546e-05,
      "loss": 4.1188,
      "step": 64
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 0.6109525561332703,
      "learning_rate": 4.0303030303030305e-05,
      "loss": 4.1289,
      "step": 65
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.5732231140136719,
      "learning_rate": 4.015151515151515e-05,
      "loss": 4.1537,
      "step": 66
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 0.5102545022964478,
      "learning_rate": 4e-05,
      "loss": 4.1524,
      "step": 67
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 0.4961165189743042,
      "learning_rate": 3.984848484848485e-05,
      "loss": 4.1236,
      "step": 68
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 0.46435195207595825,
      "learning_rate": 3.96969696969697e-05,
      "loss": 4.0399,
      "step": 69
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 0.5025492906570435,
      "learning_rate": 3.954545454545455e-05,
      "loss": 4.0723,
      "step": 70
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 0.5191460251808167,
      "learning_rate": 3.939393939393939e-05,
      "loss": 4.2118,
      "step": 71
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 0.4679144024848938,
      "learning_rate": 3.924242424242424e-05,
      "loss": 4.0398,
      "step": 72
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 0.47229281067848206,
      "learning_rate": 3.909090909090909e-05,
      "loss": 4.002,
      "step": 73
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 0.45324793457984924,
      "learning_rate": 3.8939393939393944e-05,
      "loss": 4.0645,
      "step": 74
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 0.3903409242630005,
      "learning_rate": 3.878787878787879e-05,
      "loss": 3.9885,
      "step": 75
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 0.4031064808368683,
      "learning_rate": 3.8636363636363636e-05,
      "loss": 3.9875,
      "step": 76
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.4547412395477295,
      "learning_rate": 3.848484848484848e-05,
      "loss": 4.0664,
      "step": 77
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 0.3722723126411438,
      "learning_rate": 3.8333333333333334e-05,
      "loss": 4.0338,
      "step": 78
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 0.34300294518470764,
      "learning_rate": 3.818181818181819e-05,
      "loss": 3.954,
      "step": 79
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 0.4187139570713043,
      "learning_rate": 3.803030303030303e-05,
      "loss": 4.0423,
      "step": 80
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 0.3655346632003784,
      "learning_rate": 3.787878787878788e-05,
      "loss": 3.949,
      "step": 81
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 0.33346694707870483,
      "learning_rate": 3.7727272727272725e-05,
      "loss": 3.9526,
      "step": 82
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 0.2908555865287781,
      "learning_rate": 3.757575757575758e-05,
      "loss": 3.9328,
      "step": 83
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 0.2847172021865845,
      "learning_rate": 3.742424242424243e-05,
      "loss": 3.9127,
      "step": 84
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 0.3839218318462372,
      "learning_rate": 3.7272727272727276e-05,
      "loss": 3.9562,
      "step": 85
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 0.37928223609924316,
      "learning_rate": 3.712121212121212e-05,
      "loss": 4.0982,
      "step": 86
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 0.36019834876060486,
      "learning_rate": 3.6969696969696974e-05,
      "loss": 3.9825,
      "step": 87
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.3124465346336365,
      "learning_rate": 3.681818181818182e-05,
      "loss": 3.9714,
      "step": 88
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 0.2699788510799408,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 3.9228,
      "step": 89
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 0.3079739809036255,
      "learning_rate": 3.651515151515152e-05,
      "loss": 3.9606,
      "step": 90
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 0.3957928717136383,
      "learning_rate": 3.6363636363636364e-05,
      "loss": 4.0747,
      "step": 91
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 0.29148247838020325,
      "learning_rate": 3.621212121212122e-05,
      "loss": 3.9022,
      "step": 92
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 0.2617470622062683,
      "learning_rate": 3.606060606060606e-05,
      "loss": 3.8957,
      "step": 93
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 0.32460668683052063,
      "learning_rate": 3.590909090909091e-05,
      "loss": 3.9028,
      "step": 94
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 0.38374945521354675,
      "learning_rate": 3.575757575757576e-05,
      "loss": 3.9757,
      "step": 95
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 0.23898673057556152,
      "learning_rate": 3.560606060606061e-05,
      "loss": 3.8715,
      "step": 96
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 0.3426683247089386,
      "learning_rate": 3.545454545454546e-05,
      "loss": 3.9534,
      "step": 97
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 0.37509390711784363,
      "learning_rate": 3.5303030303030305e-05,
      "loss": 3.9169,
      "step": 98
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.36817309260368347,
      "learning_rate": 3.515151515151515e-05,
      "loss": 3.9613,
      "step": 99
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 0.25571081042289734,
      "learning_rate": 3.5e-05,
      "loss": 3.8786,
      "step": 100
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 0.3582957983016968,
      "learning_rate": 3.484848484848485e-05,
      "loss": 3.9544,
      "step": 101
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 0.2982356548309326,
      "learning_rate": 3.46969696969697e-05,
      "loss": 3.8813,
      "step": 102
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 0.28359535336494446,
      "learning_rate": 3.454545454545455e-05,
      "loss": 3.8919,
      "step": 103
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 0.3785155415534973,
      "learning_rate": 3.4393939393939394e-05,
      "loss": 3.9486,
      "step": 104
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 0.30883559584617615,
      "learning_rate": 3.424242424242424e-05,
      "loss": 3.8715,
      "step": 105
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 0.3456646203994751,
      "learning_rate": 3.409090909090909e-05,
      "loss": 3.927,
      "step": 106
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 0.4306612014770508,
      "learning_rate": 3.3939393939393945e-05,
      "loss": 4.0193,
      "step": 107
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 0.338142067193985,
      "learning_rate": 3.378787878787879e-05,
      "loss": 3.8891,
      "step": 108
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 0.3322716951370239,
      "learning_rate": 3.3636363636363636e-05,
      "loss": 3.9041,
      "step": 109
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.2785416841506958,
      "learning_rate": 3.348484848484848e-05,
      "loss": 3.8516,
      "step": 110
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 0.30710315704345703,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 3.8797,
      "step": 111
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 0.42885860800743103,
      "learning_rate": 3.318181818181819e-05,
      "loss": 3.9997,
      "step": 112
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 0.3309677243232727,
      "learning_rate": 3.303030303030303e-05,
      "loss": 3.9092,
      "step": 113
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 0.3088890314102173,
      "learning_rate": 3.287878787878788e-05,
      "loss": 3.8567,
      "step": 114
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 0.3819344937801361,
      "learning_rate": 3.272727272727273e-05,
      "loss": 3.916,
      "step": 115
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 0.3108183741569519,
      "learning_rate": 3.257575757575758e-05,
      "loss": 3.8547,
      "step": 116
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 0.3688814342021942,
      "learning_rate": 3.2424242424242423e-05,
      "loss": 3.8709,
      "step": 117
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 0.43599921464920044,
      "learning_rate": 3.2272727272727276e-05,
      "loss": 3.9182,
      "step": 118
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 0.30278706550598145,
      "learning_rate": 3.212121212121212e-05,
      "loss": 3.8515,
      "step": 119
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 0.3082312047481537,
      "learning_rate": 3.1969696969696974e-05,
      "loss": 3.8358,
      "step": 120
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.36360910534858704,
      "learning_rate": 3.181818181818182e-05,
      "loss": 3.8786,
      "step": 121
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 0.33883413672447205,
      "learning_rate": 3.1666666666666666e-05,
      "loss": 3.8434,
      "step": 122
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 0.3212487995624542,
      "learning_rate": 3.151515151515151e-05,
      "loss": 3.8451,
      "step": 123
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 0.6042824983596802,
      "learning_rate": 3.1363636363636365e-05,
      "loss": 3.9581,
      "step": 124
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 0.670677900314331,
      "learning_rate": 3.121212121212122e-05,
      "loss": 3.8502,
      "step": 125
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 0.26885172724723816,
      "learning_rate": 3.106060606060606e-05,
      "loss": 3.825,
      "step": 126
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 0.4326172173023224,
      "learning_rate": 3.090909090909091e-05,
      "loss": 3.899,
      "step": 127
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 0.3371596932411194,
      "learning_rate": 3.0757575757575755e-05,
      "loss": 3.8832,
      "step": 128
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 0.35294249653816223,
      "learning_rate": 3.060606060606061e-05,
      "loss": 3.8541,
      "step": 129
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 0.36534833908081055,
      "learning_rate": 3.0454545454545456e-05,
      "loss": 3.8618,
      "step": 130
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 0.41995078325271606,
      "learning_rate": 3.0303030303030306e-05,
      "loss": 3.8917,
      "step": 131
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.3551675081253052,
      "learning_rate": 3.015151515151515e-05,
      "loss": 3.8582,
      "step": 132
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 0.5216735601425171,
      "learning_rate": 3e-05,
      "loss": 3.9309,
      "step": 133
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 0.4098893105983734,
      "learning_rate": 2.9848484848484847e-05,
      "loss": 3.8829,
      "step": 134
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 0.29401695728302,
      "learning_rate": 2.96969696969697e-05,
      "loss": 3.8298,
      "step": 135
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 0.4218105673789978,
      "learning_rate": 2.954545454545455e-05,
      "loss": 3.8769,
      "step": 136
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 0.35370996594429016,
      "learning_rate": 2.9393939393939394e-05,
      "loss": 3.8424,
      "step": 137
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 0.3799337148666382,
      "learning_rate": 2.9242424242424243e-05,
      "loss": 3.8175,
      "step": 138
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 0.4678793251514435,
      "learning_rate": 2.909090909090909e-05,
      "loss": 3.8268,
      "step": 139
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 0.43919724225997925,
      "learning_rate": 2.893939393939394e-05,
      "loss": 3.8659,
      "step": 140
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 0.5043254494667053,
      "learning_rate": 2.878787878787879e-05,
      "loss": 3.8319,
      "step": 141
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 0.952017068862915,
      "learning_rate": 2.863636363636364e-05,
      "loss": 3.8575,
      "step": 142
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.4989220201969147,
      "learning_rate": 2.8484848484848486e-05,
      "loss": 3.8424,
      "step": 143
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 0.24313996732234955,
      "learning_rate": 2.8333333333333335e-05,
      "loss": 3.8088,
      "step": 144
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 0.3993836045265198,
      "learning_rate": 2.818181818181818e-05,
      "loss": 3.8158,
      "step": 145
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 0.4494760036468506,
      "learning_rate": 2.803030303030303e-05,
      "loss": 3.8592,
      "step": 146
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 0.3654288947582245,
      "learning_rate": 2.7878787878787883e-05,
      "loss": 3.8334,
      "step": 147
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 0.3901422619819641,
      "learning_rate": 2.772727272727273e-05,
      "loss": 3.8529,
      "step": 148
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 0.5142775774002075,
      "learning_rate": 2.7575757575757578e-05,
      "loss": 3.8568,
      "step": 149
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 0.3551257252693176,
      "learning_rate": 2.7424242424242424e-05,
      "loss": 3.8211,
      "step": 150
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 0.34587082266807556,
      "learning_rate": 2.7272727272727273e-05,
      "loss": 3.8224,
      "step": 151
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 0.3480145037174225,
      "learning_rate": 2.7121212121212126e-05,
      "loss": 3.8265,
      "step": 152
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 0.516316831111908,
      "learning_rate": 2.696969696969697e-05,
      "loss": 3.8876,
      "step": 153
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.4170283079147339,
      "learning_rate": 2.681818181818182e-05,
      "loss": 3.8284,
      "step": 154
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 0.3912806510925293,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.8371,
      "step": 155
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 0.33737245202064514,
      "learning_rate": 2.6515151515151516e-05,
      "loss": 3.8153,
      "step": 156
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 0.318382203578949,
      "learning_rate": 2.636363636363636e-05,
      "loss": 3.8126,
      "step": 157
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 0.34580498933792114,
      "learning_rate": 2.6212121212121214e-05,
      "loss": 3.805,
      "step": 158
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 0.3650664985179901,
      "learning_rate": 2.6060606060606063e-05,
      "loss": 3.8049,
      "step": 159
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 0.8813711404800415,
      "learning_rate": 2.590909090909091e-05,
      "loss": 3.8427,
      "step": 160
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 0.4203108251094818,
      "learning_rate": 2.575757575757576e-05,
      "loss": 3.8188,
      "step": 161
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 0.41937166452407837,
      "learning_rate": 2.5606060606060604e-05,
      "loss": 3.8177,
      "step": 162
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 0.5165674090385437,
      "learning_rate": 2.5454545454545454e-05,
      "loss": 3.8151,
      "step": 163
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 0.5268250107765198,
      "learning_rate": 2.5303030303030306e-05,
      "loss": 3.8631,
      "step": 164
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.48981085419654846,
      "learning_rate": 2.5151515151515155e-05,
      "loss": 3.8315,
      "step": 165
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 0.5028322339057922,
      "learning_rate": 2.5e-05,
      "loss": 3.8565,
      "step": 166
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 0.31912845373153687,
      "learning_rate": 2.4848484848484847e-05,
      "loss": 3.8025,
      "step": 167
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 0.21048957109451294,
      "learning_rate": 2.46969696969697e-05,
      "loss": 3.7991,
      "step": 168
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 0.41173872351646423,
      "learning_rate": 2.4545454545454545e-05,
      "loss": 3.815,
      "step": 169
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 0.3525952696800232,
      "learning_rate": 2.4393939393939395e-05,
      "loss": 3.8082,
      "step": 170
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 0.3217739462852478,
      "learning_rate": 2.4242424242424244e-05,
      "loss": 3.8058,
      "step": 171
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 0.2606699764728546,
      "learning_rate": 2.4090909090909093e-05,
      "loss": 3.8072,
      "step": 172
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 0.3739631175994873,
      "learning_rate": 2.393939393939394e-05,
      "loss": 3.8185,
      "step": 173
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 0.27466949820518494,
      "learning_rate": 2.3787878787878788e-05,
      "loss": 3.803,
      "step": 174
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 0.4501763582229614,
      "learning_rate": 2.3636363636363637e-05,
      "loss": 3.8194,
      "step": 175
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.5599799156188965,
      "learning_rate": 2.3484848484848487e-05,
      "loss": 3.8223,
      "step": 176
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 0.27931058406829834,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 3.8037,
      "step": 177
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 0.4313260614871979,
      "learning_rate": 2.318181818181818e-05,
      "loss": 3.8033,
      "step": 178
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 0.36969804763793945,
      "learning_rate": 2.3030303030303034e-05,
      "loss": 3.8021,
      "step": 179
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 0.25409549474716187,
      "learning_rate": 2.287878787878788e-05,
      "loss": 3.7996,
      "step": 180
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 0.3588641285896301,
      "learning_rate": 2.272727272727273e-05,
      "loss": 3.8095,
      "step": 181
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 0.3333243131637573,
      "learning_rate": 2.257575757575758e-05,
      "loss": 3.8072,
      "step": 182
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 0.2981562316417694,
      "learning_rate": 2.2424242424242424e-05,
      "loss": 3.7967,
      "step": 183
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 0.23555895686149597,
      "learning_rate": 2.2272727272727274e-05,
      "loss": 3.7998,
      "step": 184
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 0.5371624231338501,
      "learning_rate": 2.2121212121212123e-05,
      "loss": 3.8194,
      "step": 185
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 0.41286903619766235,
      "learning_rate": 2.1969696969696972e-05,
      "loss": 3.7972,
      "step": 186
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.4788220226764679,
      "learning_rate": 2.1818181818181818e-05,
      "loss": 3.8078,
      "step": 187
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 0.3429073393344879,
      "learning_rate": 2.1666666666666667e-05,
      "loss": 3.8114,
      "step": 188
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 0.14653097093105316,
      "learning_rate": 2.1515151515151516e-05,
      "loss": 3.7966,
      "step": 189
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 0.48351192474365234,
      "learning_rate": 2.1363636363636362e-05,
      "loss": 3.8045,
      "step": 190
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 0.37778595089912415,
      "learning_rate": 2.1212121212121215e-05,
      "loss": 3.8041,
      "step": 191
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 0.18990322947502136,
      "learning_rate": 2.106060606060606e-05,
      "loss": 3.796,
      "step": 192
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 0.2925557792186737,
      "learning_rate": 2.090909090909091e-05,
      "loss": 3.7997,
      "step": 193
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 0.40083611011505127,
      "learning_rate": 2.075757575757576e-05,
      "loss": 3.799,
      "step": 194
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 0.3397413492202759,
      "learning_rate": 2.0606060606060608e-05,
      "loss": 3.7956,
      "step": 195
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 0.22517837584018707,
      "learning_rate": 2.0454545454545457e-05,
      "loss": 3.7937,
      "step": 196
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 0.3262883424758911,
      "learning_rate": 2.0303030303030303e-05,
      "loss": 3.7985,
      "step": 197
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.26116588711738586,
      "learning_rate": 2.0151515151515152e-05,
      "loss": 3.7988,
      "step": 198
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 0.1668681651353836,
      "learning_rate": 2e-05,
      "loss": 3.7956,
      "step": 199
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 0.26242896914482117,
      "learning_rate": 1.984848484848485e-05,
      "loss": 3.8008,
      "step": 200
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 0.26940953731536865,
      "learning_rate": 1.9696969696969697e-05,
      "loss": 3.7934,
      "step": 201
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 0.2145806849002838,
      "learning_rate": 1.9545454545454546e-05,
      "loss": 3.7927,
      "step": 202
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 0.24179664254188538,
      "learning_rate": 1.9393939393939395e-05,
      "loss": 3.7947,
      "step": 203
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 0.1964484602212906,
      "learning_rate": 1.924242424242424e-05,
      "loss": 3.7964,
      "step": 204
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 0.11244557797908783,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 3.7933,
      "step": 205
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 0.2157948911190033,
      "learning_rate": 1.893939393939394e-05,
      "loss": 3.7954,
      "step": 206
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 0.3582364618778229,
      "learning_rate": 1.878787878787879e-05,
      "loss": 3.7994,
      "step": 207
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 0.43497568368911743,
      "learning_rate": 1.8636363636363638e-05,
      "loss": 3.7983,
      "step": 208
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.24088507890701294,
      "learning_rate": 1.8484848484848487e-05,
      "loss": 3.7955,
      "step": 209
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 0.16344839334487915,
      "learning_rate": 1.8333333333333333e-05,
      "loss": 3.7957,
      "step": 210
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 0.0930844247341156,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 3.7923,
      "step": 211
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 0.22031797468662262,
      "learning_rate": 1.803030303030303e-05,
      "loss": 3.7918,
      "step": 212
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 0.09173697978258133,
      "learning_rate": 1.787878787878788e-05,
      "loss": 3.7918,
      "step": 213
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 0.19701741635799408,
      "learning_rate": 1.772727272727273e-05,
      "loss": 3.7935,
      "step": 214
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 0.308745414018631,
      "learning_rate": 1.7575757575757576e-05,
      "loss": 3.7921,
      "step": 215
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 0.14262405037879944,
      "learning_rate": 1.7424242424242425e-05,
      "loss": 3.7935,
      "step": 216
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 0.19038768112659454,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 3.7931,
      "step": 217
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 0.18292106688022614,
      "learning_rate": 1.712121212121212e-05,
      "loss": 3.7936,
      "step": 218
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 0.4053524136543274,
      "learning_rate": 1.6969696969696972e-05,
      "loss": 3.7952,
      "step": 219
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.18878206610679626,
      "learning_rate": 1.6818181818181818e-05,
      "loss": 3.7919,
      "step": 220
    },
    {
      "epoch": 20.09090909090909,
      "grad_norm": 0.15286315977573395,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 3.7916,
      "step": 221
    },
    {
      "epoch": 20.181818181818183,
      "grad_norm": 0.38003450632095337,
      "learning_rate": 1.6515151515151517e-05,
      "loss": 3.7923,
      "step": 222
    },
    {
      "epoch": 20.272727272727273,
      "grad_norm": 0.23930558562278748,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 3.794,
      "step": 223
    },
    {
      "epoch": 20.363636363636363,
      "grad_norm": 0.17180894315242767,
      "learning_rate": 1.6212121212121212e-05,
      "loss": 3.7919,
      "step": 224
    },
    {
      "epoch": 20.454545454545453,
      "grad_norm": 0.15364912152290344,
      "learning_rate": 1.606060606060606e-05,
      "loss": 3.792,
      "step": 225
    },
    {
      "epoch": 20.545454545454547,
      "grad_norm": 0.4339780807495117,
      "learning_rate": 1.590909090909091e-05,
      "loss": 3.7937,
      "step": 226
    },
    {
      "epoch": 20.636363636363637,
      "grad_norm": 0.1597868651151657,
      "learning_rate": 1.5757575757575756e-05,
      "loss": 3.7922,
      "step": 227
    },
    {
      "epoch": 20.727272727272727,
      "grad_norm": 0.3502880930900574,
      "learning_rate": 1.560606060606061e-05,
      "loss": 3.7908,
      "step": 228
    },
    {
      "epoch": 20.818181818181817,
      "grad_norm": 0.11936622858047485,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 3.7908,
      "step": 229
    },
    {
      "epoch": 20.90909090909091,
      "grad_norm": 0.07835465669631958,
      "learning_rate": 1.5303030303030304e-05,
      "loss": 3.7903,
      "step": 230
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.4241098165512085,
      "learning_rate": 1.5151515151515153e-05,
      "loss": 3.7952,
      "step": 231
    },
    {
      "epoch": 21.09090909090909,
      "grad_norm": 0.34337979555130005,
      "learning_rate": 1.5e-05,
      "loss": 3.7942,
      "step": 232
    },
    {
      "epoch": 21.181818181818183,
      "grad_norm": 0.1331956386566162,
      "learning_rate": 1.484848484848485e-05,
      "loss": 3.7905,
      "step": 233
    },
    {
      "epoch": 21.272727272727273,
      "grad_norm": 0.1719973236322403,
      "learning_rate": 1.4696969696969697e-05,
      "loss": 3.791,
      "step": 234
    },
    {
      "epoch": 21.363636363636363,
      "grad_norm": 0.1001654639840126,
      "learning_rate": 1.4545454545454545e-05,
      "loss": 3.7913,
      "step": 235
    },
    {
      "epoch": 21.454545454545453,
      "grad_norm": 0.18028581142425537,
      "learning_rate": 1.4393939393939396e-05,
      "loss": 3.7914,
      "step": 236
    },
    {
      "epoch": 21.545454545454547,
      "grad_norm": 0.06532520055770874,
      "learning_rate": 1.4242424242424243e-05,
      "loss": 3.7897,
      "step": 237
    },
    {
      "epoch": 21.636363636363637,
      "grad_norm": 0.19025860726833344,
      "learning_rate": 1.409090909090909e-05,
      "loss": 3.7894,
      "step": 238
    },
    {
      "epoch": 21.727272727272727,
      "grad_norm": 0.2399425506591797,
      "learning_rate": 1.3939393939393942e-05,
      "loss": 3.7897,
      "step": 239
    },
    {
      "epoch": 21.818181818181817,
      "grad_norm": 0.12965510785579681,
      "learning_rate": 1.3787878787878789e-05,
      "loss": 3.7902,
      "step": 240
    },
    {
      "epoch": 21.90909090909091,
      "grad_norm": 0.18967998027801514,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 3.7912,
      "step": 241
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.5707873702049255,
      "learning_rate": 1.3484848484848486e-05,
      "loss": 3.7941,
      "step": 242
    },
    {
      "epoch": 22.09090909090909,
      "grad_norm": 0.1164667159318924,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 3.7887,
      "step": 243
    },
    {
      "epoch": 22.181818181818183,
      "grad_norm": 0.33074891567230225,
      "learning_rate": 1.318181818181818e-05,
      "loss": 3.7906,
      "step": 244
    },
    {
      "epoch": 22.272727272727273,
      "grad_norm": 0.11165686696767807,
      "learning_rate": 1.3030303030303032e-05,
      "loss": 3.7896,
      "step": 245
    },
    {
      "epoch": 22.363636363636363,
      "grad_norm": 0.1377628594636917,
      "learning_rate": 1.287878787878788e-05,
      "loss": 3.7885,
      "step": 246
    },
    {
      "epoch": 22.454545454545453,
      "grad_norm": 0.060392457991838455,
      "learning_rate": 1.2727272727272727e-05,
      "loss": 3.789,
      "step": 247
    },
    {
      "epoch": 22.545454545454547,
      "grad_norm": 0.12597247958183289,
      "learning_rate": 1.2575757575757578e-05,
      "loss": 3.7902,
      "step": 248
    },
    {
      "epoch": 22.636363636363637,
      "grad_norm": 0.24055595695972443,
      "learning_rate": 1.2424242424242424e-05,
      "loss": 3.7904,
      "step": 249
    },
    {
      "epoch": 22.727272727272727,
      "grad_norm": 0.3253293037414551,
      "learning_rate": 1.2272727272727273e-05,
      "loss": 3.7917,
      "step": 250
    },
    {
      "epoch": 22.818181818181817,
      "grad_norm": 0.17484928667545319,
      "learning_rate": 1.2121212121212122e-05,
      "loss": 3.7907,
      "step": 251
    },
    {
      "epoch": 22.90909090909091,
      "grad_norm": 0.20295564830303192,
      "learning_rate": 1.196969696969697e-05,
      "loss": 3.7904,
      "step": 252
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.20536282658576965,
      "learning_rate": 1.1818181818181819e-05,
      "loss": 3.7904,
      "step": 253
    },
    {
      "epoch": 23.09090909090909,
      "grad_norm": 0.2006571739912033,
      "learning_rate": 1.1666666666666668e-05,
      "loss": 3.7903,
      "step": 254
    },
    {
      "epoch": 23.181818181818183,
      "grad_norm": 0.10811205953359604,
      "learning_rate": 1.1515151515151517e-05,
      "loss": 3.7896,
      "step": 255
    },
    {
      "epoch": 23.272727272727273,
      "grad_norm": 0.05282926931977272,
      "learning_rate": 1.1363636363636365e-05,
      "loss": 3.7884,
      "step": 256
    },
    {
      "epoch": 23.363636363636363,
      "grad_norm": 0.12655331194400787,
      "learning_rate": 1.1212121212121212e-05,
      "loss": 3.7892,
      "step": 257
    },
    {
      "epoch": 23.454545454545453,
      "grad_norm": 0.11008144915103912,
      "learning_rate": 1.1060606060606061e-05,
      "loss": 3.7883,
      "step": 258
    },
    {
      "epoch": 23.545454545454547,
      "grad_norm": 0.13709424436092377,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 3.7896,
      "step": 259
    },
    {
      "epoch": 23.636363636363637,
      "grad_norm": 0.09698457270860672,
      "learning_rate": 1.0757575757575758e-05,
      "loss": 3.7887,
      "step": 260
    },
    {
      "epoch": 23.727272727272727,
      "grad_norm": 0.11090438067913055,
      "learning_rate": 1.0606060606060607e-05,
      "loss": 3.7879,
      "step": 261
    },
    {
      "epoch": 23.818181818181817,
      "grad_norm": 0.25061389803886414,
      "learning_rate": 1.0454545454545455e-05,
      "loss": 3.7904,
      "step": 262
    },
    {
      "epoch": 23.90909090909091,
      "grad_norm": 0.15984803438186646,
      "learning_rate": 1.0303030303030304e-05,
      "loss": 3.7887,
      "step": 263
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.10847331583499908,
      "learning_rate": 1.0151515151515152e-05,
      "loss": 3.7895,
      "step": 264
    },
    {
      "epoch": 24.09090909090909,
      "grad_norm": 0.11252561956644058,
      "learning_rate": 1e-05,
      "loss": 3.7891,
      "step": 265
    },
    {
      "epoch": 24.181818181818183,
      "grad_norm": 0.09418612718582153,
      "learning_rate": 9.848484848484848e-06,
      "loss": 3.7885,
      "step": 266
    },
    {
      "epoch": 24.272727272727273,
      "grad_norm": 0.1400700807571411,
      "learning_rate": 9.696969696969698e-06,
      "loss": 3.7888,
      "step": 267
    },
    {
      "epoch": 24.363636363636363,
      "grad_norm": 0.07728955149650574,
      "learning_rate": 9.545454545454547e-06,
      "loss": 3.7881,
      "step": 268
    },
    {
      "epoch": 24.454545454545453,
      "grad_norm": 0.08829490840435028,
      "learning_rate": 9.393939393939394e-06,
      "loss": 3.7881,
      "step": 269
    },
    {
      "epoch": 24.545454545454547,
      "grad_norm": 0.11022491753101349,
      "learning_rate": 9.242424242424244e-06,
      "loss": 3.7891,
      "step": 270
    },
    {
      "epoch": 24.636363636363637,
      "grad_norm": 0.10001656413078308,
      "learning_rate": 9.090909090909091e-06,
      "loss": 3.7873,
      "step": 271
    },
    {
      "epoch": 24.727272727272727,
      "grad_norm": 0.2110518217086792,
      "learning_rate": 8.93939393939394e-06,
      "loss": 3.7896,
      "step": 272
    },
    {
      "epoch": 24.818181818181817,
      "grad_norm": 0.11588362604379654,
      "learning_rate": 8.787878787878788e-06,
      "loss": 3.7874,
      "step": 273
    },
    {
      "epoch": 24.90909090909091,
      "grad_norm": 0.07995198667049408,
      "learning_rate": 8.636363636363637e-06,
      "loss": 3.7886,
      "step": 274
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.04896334186196327,
      "learning_rate": 8.484848484848486e-06,
      "loss": 3.7877,
      "step": 275
    },
    {
      "epoch": 25.09090909090909,
      "grad_norm": 0.16137120127677917,
      "learning_rate": 8.333333333333334e-06,
      "loss": 3.789,
      "step": 276
    },
    {
      "epoch": 25.181818181818183,
      "grad_norm": 0.10744916647672653,
      "learning_rate": 8.181818181818183e-06,
      "loss": 3.7887,
      "step": 277
    },
    {
      "epoch": 25.272727272727273,
      "grad_norm": 0.08707313239574432,
      "learning_rate": 8.03030303030303e-06,
      "loss": 3.7879,
      "step": 278
    },
    {
      "epoch": 25.363636363636363,
      "grad_norm": 0.08724983036518097,
      "learning_rate": 7.878787878787878e-06,
      "loss": 3.7882,
      "step": 279
    },
    {
      "epoch": 25.454545454545453,
      "grad_norm": 0.08182623237371445,
      "learning_rate": 7.727272727272727e-06,
      "loss": 3.7884,
      "step": 280
    },
    {
      "epoch": 25.545454545454547,
      "grad_norm": 0.0708923414349556,
      "learning_rate": 7.5757575757575764e-06,
      "loss": 3.7877,
      "step": 281
    },
    {
      "epoch": 25.636363636363637,
      "grad_norm": 0.18868747353553772,
      "learning_rate": 7.424242424242425e-06,
      "loss": 3.7894,
      "step": 282
    },
    {
      "epoch": 25.727272727272727,
      "grad_norm": 0.08868715167045593,
      "learning_rate": 7.272727272727272e-06,
      "loss": 3.7869,
      "step": 283
    },
    {
      "epoch": 25.818181818181817,
      "grad_norm": 0.08047348260879517,
      "learning_rate": 7.1212121212121215e-06,
      "loss": 3.7875,
      "step": 284
    },
    {
      "epoch": 25.90909090909091,
      "grad_norm": 0.07549051195383072,
      "learning_rate": 6.969696969696971e-06,
      "loss": 3.7868,
      "step": 285
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.04774728789925575,
      "learning_rate": 6.818181818181818e-06,
      "loss": 3.7873,
      "step": 286
    },
    {
      "epoch": 26.09090909090909,
      "grad_norm": 0.07735054194927216,
      "learning_rate": 6.666666666666667e-06,
      "loss": 3.7881,
      "step": 287
    },
    {
      "epoch": 26.181818181818183,
      "grad_norm": 0.04647233337163925,
      "learning_rate": 6.515151515151516e-06,
      "loss": 3.7873,
      "step": 288
    },
    {
      "epoch": 26.272727272727273,
      "grad_norm": 0.08687634021043777,
      "learning_rate": 6.363636363636363e-06,
      "loss": 3.7879,
      "step": 289
    },
    {
      "epoch": 26.363636363636363,
      "grad_norm": 0.07174123078584671,
      "learning_rate": 6.212121212121212e-06,
      "loss": 3.7866,
      "step": 290
    },
    {
      "epoch": 26.454545454545453,
      "grad_norm": 0.19055193662643433,
      "learning_rate": 6.060606060606061e-06,
      "loss": 3.7887,
      "step": 291
    },
    {
      "epoch": 26.545454545454547,
      "grad_norm": 0.08447420597076416,
      "learning_rate": 5.909090909090909e-06,
      "loss": 3.7876,
      "step": 292
    },
    {
      "epoch": 26.636363636363637,
      "grad_norm": 0.06614471226930618,
      "learning_rate": 5.7575757575757586e-06,
      "loss": 3.7873,
      "step": 293
    },
    {
      "epoch": 26.727272727272727,
      "grad_norm": 0.09659640491008759,
      "learning_rate": 5.606060606060606e-06,
      "loss": 3.7867,
      "step": 294
    },
    {
      "epoch": 26.818181818181817,
      "grad_norm": 0.06712604314088821,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 3.7872,
      "step": 295
    },
    {
      "epoch": 26.90909090909091,
      "grad_norm": 0.1035582423210144,
      "learning_rate": 5.303030303030304e-06,
      "loss": 3.7881,
      "step": 296
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.11285149306058884,
      "learning_rate": 5.151515151515152e-06,
      "loss": 3.788,
      "step": 297
    },
    {
      "epoch": 27.09090909090909,
      "grad_norm": 0.07244466990232468,
      "learning_rate": 5e-06,
      "loss": 3.7871,
      "step": 298
    },
    {
      "epoch": 27.181818181818183,
      "grad_norm": 0.062423814088106155,
      "learning_rate": 4.848484848484849e-06,
      "loss": 3.7871,
      "step": 299
    },
    {
      "epoch": 27.272727272727273,
      "grad_norm": 0.09204698354005814,
      "learning_rate": 4.696969696969697e-06,
      "loss": 3.7879,
      "step": 300
    },
    {
      "epoch": 27.363636363636363,
      "grad_norm": 0.06402111798524857,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 3.7863,
      "step": 301
    },
    {
      "epoch": 27.454545454545453,
      "grad_norm": 0.06911343336105347,
      "learning_rate": 4.393939393939394e-06,
      "loss": 3.7877,
      "step": 302
    },
    {
      "epoch": 27.545454545454547,
      "grad_norm": 0.1677778959274292,
      "learning_rate": 4.242424242424243e-06,
      "loss": 3.7881,
      "step": 303
    },
    {
      "epoch": 27.636363636363637,
      "grad_norm": 0.09282161295413971,
      "learning_rate": 4.0909090909090915e-06,
      "loss": 3.7877,
      "step": 304
    },
    {
      "epoch": 27.727272727272727,
      "grad_norm": 0.1069866493344307,
      "learning_rate": 3.939393939393939e-06,
      "loss": 3.7878,
      "step": 305
    },
    {
      "epoch": 27.818181818181817,
      "grad_norm": 0.10750395804643631,
      "learning_rate": 3.7878787878787882e-06,
      "loss": 3.788,
      "step": 306
    },
    {
      "epoch": 27.90909090909091,
      "grad_norm": 0.09656523913145065,
      "learning_rate": 3.636363636363636e-06,
      "loss": 3.7865,
      "step": 307
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.04499923065304756,
      "learning_rate": 3.4848484848484854e-06,
      "loss": 3.7869,
      "step": 308
    },
    {
      "epoch": 28.09090909090909,
      "grad_norm": 0.10106971859931946,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 3.7879,
      "step": 309
    },
    {
      "epoch": 28.181818181818183,
      "grad_norm": 0.07092998921871185,
      "learning_rate": 3.1818181818181817e-06,
      "loss": 3.7869,
      "step": 310
    },
    {
      "epoch": 28.272727272727273,
      "grad_norm": 0.06086558848619461,
      "learning_rate": 3.0303030303030305e-06,
      "loss": 3.7869,
      "step": 311
    },
    {
      "epoch": 28.363636363636363,
      "grad_norm": 0.07800531387329102,
      "learning_rate": 2.8787878787878793e-06,
      "loss": 3.7872,
      "step": 312
    },
    {
      "epoch": 28.454545454545453,
      "grad_norm": 0.08533897995948792,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 3.7863,
      "step": 313
    },
    {
      "epoch": 28.545454545454547,
      "grad_norm": 0.07500586658716202,
      "learning_rate": 2.575757575757576e-06,
      "loss": 3.7875,
      "step": 314
    },
    {
      "epoch": 28.636363636363637,
      "grad_norm": 0.062367066740989685,
      "learning_rate": 2.4242424242424244e-06,
      "loss": 3.7862,
      "step": 315
    },
    {
      "epoch": 28.727272727272727,
      "grad_norm": 0.04469521343708038,
      "learning_rate": 2.2727272727272728e-06,
      "loss": 3.7868,
      "step": 316
    },
    {
      "epoch": 28.818181818181817,
      "grad_norm": 0.0879129022359848,
      "learning_rate": 2.1212121212121216e-06,
      "loss": 3.7875,
      "step": 317
    },
    {
      "epoch": 28.90909090909091,
      "grad_norm": 0.16984252631664276,
      "learning_rate": 1.9696969696969695e-06,
      "loss": 3.7879,
      "step": 318
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.10561995208263397,
      "learning_rate": 1.818181818181818e-06,
      "loss": 3.7876,
      "step": 319
    },
    {
      "epoch": 29.09090909090909,
      "grad_norm": 0.04419584199786186,
      "learning_rate": 1.6666666666666667e-06,
      "loss": 3.7868,
      "step": 320
    },
    {
      "epoch": 29.181818181818183,
      "grad_norm": 0.0896906703710556,
      "learning_rate": 1.5151515151515152e-06,
      "loss": 3.7875,
      "step": 321
    },
    {
      "epoch": 29.272727272727273,
      "grad_norm": 0.08364666998386383,
      "learning_rate": 1.3636363636363636e-06,
      "loss": 3.7876,
      "step": 322
    },
    {
      "epoch": 29.363636363636363,
      "grad_norm": 0.08169877529144287,
      "learning_rate": 1.2121212121212122e-06,
      "loss": 3.7875,
      "step": 323
    },
    {
      "epoch": 29.454545454545453,
      "grad_norm": 0.05765930190682411,
      "learning_rate": 1.0606060606060608e-06,
      "loss": 3.7868,
      "step": 324
    },
    {
      "epoch": 29.545454545454547,
      "grad_norm": 0.10347035527229309,
      "learning_rate": 9.09090909090909e-07,
      "loss": 3.7876,
      "step": 325
    },
    {
      "epoch": 29.636363636363637,
      "grad_norm": 0.08310482650995255,
      "learning_rate": 7.575757575757576e-07,
      "loss": 3.7868,
      "step": 326
    },
    {
      "epoch": 29.727272727272727,
      "grad_norm": 0.07743450999259949,
      "learning_rate": 6.060606060606061e-07,
      "loss": 3.7871,
      "step": 327
    },
    {
      "epoch": 29.818181818181817,
      "grad_norm": 0.16486039757728577,
      "learning_rate": 4.545454545454545e-07,
      "loss": 3.7878,
      "step": 328
    },
    {
      "epoch": 29.90909090909091,
      "grad_norm": 0.05780128017067909,
      "learning_rate": 3.0303030303030305e-07,
      "loss": 3.7861,
      "step": 329
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.08094195276498795,
      "learning_rate": 1.5151515151515152e-07,
      "loss": 3.7862,
      "step": 330
    }
  ],
  "logging_steps": 1,
  "max_steps": 330,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7019344051568640.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
