## 模型評估

評估 LLM 模型對 `qa_dataset.csv` 中問題的回答質量。

## 功能特點

1. **完整評估流程**：從加載模型配置、初始化 RAG 引擎到生成回答和計算評估指標
2. **多種評估指標**：包含 BLEU、ROUGE、METEOR 等多種常用於自然語言評估的指標
3. **進度顯示和日誌記錄**：使用 tqdm 顯示進度條，詳細的日誌記錄
4. **靈活的篩選選項**：可按類別篩選問題、限制測試數量
5. **RAG 資源追蹤**：記錄每個問題使用了哪些參考文檔
6. **結果保存與統計**：自動保存 CSV 結果文件，並輸出評估摘要統計

## 使用方法


```bash
# 使用 Qwen2.5-14B_MiniLM 配置評估所有問題
python -m Eval.model_eval_script --config_name "Qwen2.5-14B_MiniLM"

# 僅評估「藥劑」類別的前 10 個問題
python -m Eval.model_eval_script --config_name "Qwen2.5-14B_MiniLM" --category "藥劑" --limit 10

# 不使用 RAG 功能進行測試
python -m Eval.model_eval_script --config_name "Qwen2.5-14B_MiniLM" --no_rag
```

## 腳本參數說明

- `--config_name`：要評估的模型配置名稱（必需）
- `--config_path`：模型配置文件路徑（默認: `./Eval/model_config.json`）
- `--dataset_path`：問答數據集路徑（默認: `./Eval/qa_dataset.csv`）
- `--output_dir`：結果輸出目錄（默認: `./results`）
- `--category`：篩選特定類別的問題（可選）
- `--limit`：限制評估的問題數量（可選）
- `--no_rag`：不使用 RAG 功能進行測試（可選）

## 評估結果

評估結果將保存為 CSV 文件，包含以下信息：
- 原始問題和參考答案
- LLM 生成的回答
- 各項評估指標（BLEU, ROUGE, METEOR 等）
- 回答生成時間
- 使用的 RAG 資源
- 預留的人工評估欄位


## ERAS QA 模型評估結果視覺化

全面的視覺化腳本，可將模型評估結果轉換為多種直觀的圖表。這個腳本能夠產生高質量的圖表，協助您深入分析模型表現，並支援多模型結果的比較。

### 使用方法

```bash
python -m Eval.visualization-script --result_files ./Eval/results/Qwen2.5-14B_with_rag_with_rag_20250517_204558.csv
```

### 參數說明

- `--result_files`：要分析的評估結果CSV檔案路徑，可以指定多個檔案進行比較
- `--output_dir`：視覺化圖表的輸出目錄（預設：./visualizations）

### 生成的視覺化圖表

該腳本會生成以下多種視覺化圖表：

1. **指標概覽比較**
   - 直觀展示各模型在所有評估指標（BLEU-1/2/3/4、ROUGE-1/2/L、METEOR）上的平均表現
   - 通過長條圖方式並排顯示，方便直接對比

2. **回應時間比較**
   - 展示各模型處理問題的回應時間分佈
   - 使用箱形圖顯示中位數、四分位數及離群值

3. **不同類別的表現比較**
   - 分析各模型在不同問題類別下的表現差異
   - 突出顯示模型在特定領域的強項和弱項

4. **回應長度分析**
   - 分析回應長度分佈情況
   - 探索回應長度與評估指標的相關性

5. **RAG 引用資源分析**
   - 針對使用 RAG 的模型，分析最常被引用的資源文件
   - 提供資源使用頻率的直觀視圖

6. **指標雷達圖**
   - 使用雷達圖全面比較多個模型在各指標上的表現
   - 快速識別模型的優勢和不足

7. **模型弱點分析**
   - 識別模型表現最差的問題
   - 分析這些問題的類別分佈，找出模型的薄弱環節

8. **指標關係散點圖**
   - 分析不同評估指標之間的相關性
   - 通過散點圖直觀顯示模型在多個指標維度的表現
